parquet-writer {
  writeParallelism = 2
  writeParallelism = ${?WRITE_PARALLELISM}
  timeBucketTopic = "result-time-buckets"
  timeBucketTopic = ${?TIME_BUCKET_RESULT_TOPIC}
  dataTopic = "encrypted-multiple-bar-android"
  dataTopic = ${?DATA_TOPIC}
  prometheusPort = 9095
  prometheusPort = ${?PARQUET_WRITER_PROMETHEUS_PORT}
  timeBucketConsumerGroup = "parquet-writer-time-bucket-group"
  timeBucketConsumerGroup = ${?TIME_BUCKETS_CONSUMER_GROUP}
  barAndroidConsumerGroup = "parquet-writer-bar-android-group"
  barAndroidConsumerGroup = ${?BAR_ANDROID_CONSUMER_GROUP}
  envPrefix = "KAFKA_CLIENT_"
  envPrefix = ${?KAFKA_CLIENT_ENV_PREFIX}
  autoOffsetResetConfig = "earliest"
  autoOffsetResetConfig = ${?AUTO_OFFSET_RESET}
}

cos {
  url = "cos://ingest-parquet.icm-analytics-ingest-poc-storage-aj/"
  url = ${?COS_URL}
  prefix = "fs.cos.icm-analytics-ingest-poc-storage-aj"
  prefix = ${?COS_PREFIX}
  endpoint = "s3.eu-de.cloud-object-storage.appdomain.cloud"
  endpoint = ${?COS_ENDPOINT}
  cosImpl = "com.ibm.stocator.fs.ObjectStoreFileSystem"
  schemeList = "cos"
  schema = "cos"
  clientImpl = "com.ibm.stocator.fs.cos.COSAPIClient"
  apiKey = "_G8y6H6qPO1TwUTY22dYwf0C1ZhkwVHoR0GgsejCJJ7M"
  apiKey = ${?COS_API_KEY}
  serviceId = "ServiceId-676ea560-ca39-4bd4-97bf-40bea13d7bbe"
  serviceId = ${?COS_SERVICE_ID}
  accessKey = "bd452cc4c6934bd2bb00a7a314efe85a"
  accessKey = ${?COS_ACCESS_KEY}
  secretKey = "4ba52e882c63e7d6079d2cdfeb3c3cd9658784071ccc3972"
  secretKey = ${?COS_SECRET_KEY}
}

akka {
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  loglevel = "DEBUG"
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
}

# // #consumer-settings
# Properties for akka.kafka.ConsumerSettings can be
# defined in this section or a configuration section with
# the same layout.
akka.kafka {
  consumer {
    # Tuning property of scheduled polls.
    # Controls the interval from one scheduled poll to the next.
    poll-interval = 50ms

    # Tuning property of the `KafkaConsumer.poll` parameter.
    # Note that non-zero value means that the thread that
    # is executing the stage will be blocked. See also the `wakup-timeout` setting below.
    poll-timeout = 50ms

    # The stage will delay stopping the internal actor to allow processing of
    # messages already in the stream (required for successful committing).
    # Prefer use of `DrainingControl` over a large stop-timeout.
    stop-timeout = 30s

    # Duration to wait for `KafkaConsumer.close` to finish.
    close-timeout = 20s

    # If offset commit requests are not completed within this timeout
    # the returned Future is completed `CommitTimeoutException`.
    # The `Transactional.source` waits this ammount of time for the producer to mark messages as not
    # being in flight anymore as well as waiting for messages to drain, when rebalance is triggered.
    commit-timeout = 15s

    # If commits take longer than this time a warning is logged
    commit-time-warning = 1s

    # Not used anymore (since 1.0-RC1)
    # wakeup-timeout = 3s

    # Not used anymore (since 1.0-RC1)
    # max-wakeups = 10

    # If set to a finite duration, the consumer will re-send the last committed offsets periodically
    # for all assigned partitions. See https://issues.apache.org/jira/browse/KAFKA-4682.
    commit-refresh-interval = infinite

    # Not used anymore (since 1.0-RC1)
    # wakeup-debug = true

    # Fully qualified config path which holds the dispatcher configuration
    # to be used by the KafkaConsumerActor. Some blocking may occur.
    use-dispatcher = "akka.kafka.default-dispatcher"

    # Properties defined by org.apache.kafka.clients.consumer.ConsumerConfig
    # can be defined in this configuration section.
    kafka-clients {
      key.serializer = org.apache.kafka.common.serialization.StringSerializer
      value.serializer = org.apache.kafka.common.serialization.StringSerializer
      bootstrap.servers = "broker-4-gckm0kfc730hv6kt.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-2-gckm0kfc730hv6kt.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-1-gckm0kfc730hv6kt.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-5-gckm0kfc730hv6kt.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-3-gckm0kfc730hv6kt.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-0-gckm0kfc730hv6kt.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093"
      bootstrap.servers = ${?KAFKA_BOOTSTRAP_SERVERS}
      client.id = wheather-analytics-poc-consumer
      acks = -1
      security.protocol = SASL_SSL
      sasl.mechanism = PLAIN
      ssl.protocol = TLSv1.2
      ssl.enabled.protocols = TLSv1.2
      ssl.endpoint.identification.algorithm = HTTPS
      sasl.jaas.config = "org.apache.kafka.common.security.plain.PlainLoginModule required username=token password=KOq2MdESpEMzu9xvlzIBzfWJdQmjM8spp5YNt83ZNiYe;"
      sasl.jaas.config = ${?KAFKA_SASL_JAAS_CONFIG}
      client.dns.lookup = use_all_dns_ips
      # Disable auto-commit by default
      enable.auto.commit = false
      compression.type = gzip
      enable.idempotence = true
    }
    kafka-clients = ${?KAFKA_CLIENTS}

    # Time to wait for pending requests when a partition is closed
    wait-close-partition = 500ms

    # Limits the query to Kafka for a topic's position
    position-timeout = 5s

    # When using `AssignmentOffsetsForTimes` subscriptions: timeout for the
    # call to Kafka's API
    offset-for-times-timeout = 5s

    # Timeout for akka.kafka.Metadata requests
    # This value is used instead of Kafka's default from `default.api.timeout.ms`
    # which is 1 minute.
    metadata-request-timeout = 5s

    # Interval for checking that transaction was completed before closing the consumer.
    # Used in the transactional flow for exactly-once-semantics processing.
    eos-draining-check-interval = 30ms
  }


  # Properties for akka.kafka.ProducerSettings can be
  # defined in this section or a configuration section with
  # the same layout.
  producer {
    # Tuning parameter of how many sends that can run in parallel.
    parallelism = 100

    # Duration to wait for `KafkaConsumer.close` to finish.
    close-timeout = 60s

    # Fully qualified config path which holds the dispatcher configuration
    # to be used by the producer stages. Some blocking may occur.
    # When this value is empty, the dispatcher configured for the stream
    # will be used.
    use-dispatcher = "akka.kafka.default-dispatcher"

    # The time interval to commit a transaction when using the `Transactional.sink` or `Transactional.flow`
    # for exactly-once-semantics processing.
    eos-commit-interval = 100ms

    # Properties defined by org.apache.kafka.clients.producer.ProducerConfig
    # can be defined in this configuration section.
    kafka-clients = ${akka.kafka.consumer.kafka-clients}
  }
}